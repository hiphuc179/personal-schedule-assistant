import json
import os
import re
import unicodedata
import json
from typing import Optional

try:
    from underthesea import word_tokenize
    HAS_UNDERTHESEA = True
except ImportError:
    HAS_UNDERTHESEA = False


class Preprocessor:
    """Preprocessor: Chu·∫©n h√≥a, d·ªãch v√† kh√¥i ph·ª•c d·∫•u vƒÉn b·∫£n ti·∫øng Vi·ªát."""

    def __init__(self):
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(self.base_dir, "data")
        
        self.replace_dict = self._load_json("replace_dict.json")
        self.en_vi_dict = self._load_json("en_vi.json")
        self.ambiguity_dict = self._load_json("ambiguity.json")
        
    def _load_json(self, file_name: str) -> dict:
        """T·∫£i t·ª´ ƒëi·ªÉn JSON t·ª´ th∆∞ m·ª•c data."""
        path = os.path.join(self.data_dir, file_name)
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                print(f"‚ùå Error loading {file_name}: {e}")
        return {}

    def _basic_normalize(self, text: str) -> str:
        """Chu·∫©n h√≥a: chuy·ªÉn th∆∞·ªùng + NFC."""
        if not text:
            return ""
        return unicodedata.normalize("NFC", text.lower())

    def _remove_diacritics(self, text: str) -> str:
        """Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi parser."""
        if not text:
            return ""
        nfkd = unicodedata.normalize('NFKD', text)
        return ''.join(c for c in nfkd if unicodedata.category(c) != 'Mn')

    def _apply_dict_translation(self, text: str, dictionary: dict) -> str:
        """√Åp d·ª•ng d·ªãch t·ª´ ƒëi·ªÉn v·ªõi so kh·ªõp ranh gi·ªõi t·ª´."""
        if not dictionary:
            return text
            
        sorted_keys = sorted(dictionary.keys(), key=len, reverse=True)
        for key in sorted_keys:
            pattern = r'(?<!\w)' + re.escape(key) + r'(?!\w)'
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            
            for match in reversed(matches):
                start, end = match.span()
                text = text[:start] + dictionary[key] + text[end:]
        
        return text
    
    def _segment_words(self, text: str) -> str:
        """T√°ch t·ª´ s·ª≠ d·ª•ng underthesea n·∫øu c√≥ s·∫µn."""
        if HAS_UNDERTHESEA:
            try:
                tokens = word_tokenize(text)
                return " ".join(tokens).replace("_", " ")
            except Exception:
                pass
        return text
    # Trong file preprocessor.py

    def process_lite(self, text: str) -> str:
        """[M·ªöI] X·ª≠ l√Ω nh·∫π: Ch·ªâ s·ª≠a teencode/d·∫•u, KH√îNG t√°ch t·ª´ (ƒë·ªÉ gi·ªØ format gi·ªù 9:30)."""
        if not text:
            return ""

        text = self._basic_normalize(text)
        text = self._apply_dict_translation(text, self.ambiguity_dict)
        text = self._apply_dict_translation(text, self.en_vi_dict)
        text = self._apply_dict_translation(text, self.replace_dict)
        
        # [QUAN TR·ªåNG] KH√îNG G·ªåI self._segment_words(text) ·ªû ƒê√ÇY
        
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    def process(self, text: str) -> str:
        """Pipeline: chu·∫©n h√≥a ‚Üí nh·∫≠p nh·∫±ng ‚Üí anh-vi·ªát ‚Üí teencode ‚Üí t√°ch t·ª´."""
        if not text:
            return ""

        text = self._basic_normalize(text)
        text = self._apply_dict_translation(text, self.ambiguity_dict)
        text = self._apply_dict_translation(text, self.en_vi_dict)
        text = self._apply_dict_translation(text, self.replace_dict)
        text = self._segment_words(text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def process_for_parsers(self, text: str) -> str:
        """X·ª≠ l√Ω vƒÉn b·∫£n kh√¥ng d·∫•u d√†nh cho c√°c parser."""
        normalized = self.process(text)
        return self._remove_diacritics(normalized)

    def humanize(self, text: Optional[str]) -> Optional[str]:
        """Kh√¥i ph·ª•c d·∫•u t·ª´ t·∫•t c·∫£ c√°c t·ª´ ƒëi·ªÉn."""
        if not text:
            return text
        
        text = self._apply_dict_translation(text, self.ambiguity_dict)
        text = self._apply_dict_translation(text, self.en_vi_dict)
        text = self._apply_dict_translation(text, self.replace_dict)
        
        return text


if __name__ == "__main__":
    p = Preprocessor()
    
    print("\n" + "="*120)
    print("üöÄ TEST PREPROCESSOR")
    print("="*120 + "\n")

    test_cases = {
        "TEENCODE": [
            "Ko dC daU nHa",
            "tOi nAy di da banH vs B",
            "mK thich aN pHo",
        ],
        "CONTEXT": [
            "HN tui di an cuoi",
            "mai tui ve Hn an tet",
        ],
        "ENGLISH": [
            "Set cai Meeting gap",
            "Boss Confirm chua",
            "Call cho mk gap",
            "t√¥i l√†m b√†i t·ª´ n√£y gi·ªù c≈©ng ƒë∆∞·ª£c 10 ti·∫øng r·ªìi",
            "ƒëi b·ªô bu·ªïi s√°ng m√µi ng√†y",
            "t2 tuan sau nop bao cao o phong 302",
            "H·ªçp team online"
        ]
    }

    print(f"{'INPUT':<40} | {'PROCESS':<40} | {'HUMANIZE':<40}")
    print("-" * 125)
    
    for category, cases in test_cases.items():
        print(f"\n--- {category} ---")
        for text in cases:
            processed = p.process(text)
            humanized = p.humanize(processed)
            print(f"{text:<40} | {processed:<40} | {humanized:<40}")