import json
import os
import re
from typing import List, Optional, Set


class LocationParser:
    """Tr√≠ch xu·∫•t ƒë·ªãa ƒëi·ªÉm t·ª´ c√¢u ti·∫øng Vi·ªát."""
    
    def __init__(self):
        self.locations_db = self._load_locations()
        
        self.stop_verbs: Set[str] = {
            "mua", "b√°n", "thu√™", "ƒÉn", "u·ªëng", "ch∆°i", "ng·ªß", "ngh·ªâ",
            "t·∫Øm", "v·ªá", "l√†m", "ki·∫øm", "qu·∫©y", "ƒëi", "ƒë√°", "t·∫≠p",
            "xem", "s·ª≠a", "kh√°m", "ch·ªØa", "tuy·ªÉn", "thƒÉm", "ƒë√≥n", "r∆∞·ªõc",
            "g·∫∑p", "ch·ªù", "ƒë·ª£i", "l·∫•y", "n·ªôp", "ƒë√¥ng", "gi·∫£m", "h√≥ng",
            "r√∫t", "k·∫πt", "thi", "l·ªôi", "check", "coi", "nh√¨n", "th·∫•y",
            "ch·ª•p", "qu√°", "t·∫£i", "l·∫Øm", "nh∆∞", "tr√°nh", "ch·∫°y", "h√°t",
            "h√≤", "ƒë√°nh", "t√¨m", "c·∫•t", "la", "m·∫Øng", "ch·ª≠i", "v√†o", "ra",
            "l√™n", "xu·ªëng", "bi·∫øt", "hi·ªÉu", "d√°m", "th√®m", "∆∞a", "ng√°n",
            "nh·ªõ", "qu√™n", "g·ª≠i", "b∆°i", "gi·ªØ",
        }
        
        self.black_list: Set[str] = {
            "ng·ªß th√¥i", "ch∆°i nh√©", "ngh·ªâ ng∆°i", "v·ªá sinh", "l√†m vi·ªác",
            "h·ªçc b√†i", "t·∫Øm r·ª≠a", "ki·∫øm ti·ªÅn", "ƒë√¢u ƒë√≥", "ƒë√¢u", "nh√©",
            "nha", "th√¥i", "lu√¥n", "r·ªìi", "ngay", "m·∫°ng", "l√≤ng", "v·∫ª",
            "ƒë·ªì", "m∆°", "vi·ªác", "chuy·ªán", "ng∆∞·ªùi", "l√™n m·∫°ng", "trong l√≤ng",
            "ra v·∫ª", "l√™n ƒë·ªì", "trong m∆°", "v√†o vi·ªác", "ta ƒë√¢y",
            "ng∆∞·ªùi h√¢m m·ªô", "qua loa", "b·ªô", "bu·ªïi", "s√°ng", "tr∆∞a",
            "chi·ªÅu", "t·ªëi", "ƒë√™m", "khuya", "h√¥m nay", "ng√†y mai", "m·ªët",
            "tu·∫ßn", "th√°ng", "nƒÉm", "th·ª© 2", "th·ª© 3", "th·ª© 4", "th·ª© 5",
            "th·ª© 6", "th·ª© 7", "ch·ªß nh·∫≠t", "cn",
            
        }
        
        self.prep_pattern = re.compile(
            r"(?:t·∫°i|·ªü|ƒë·∫øn|v·ªÅ|gh√©|ra|trong|tr√™n|t·ªõi|l√™n|xu·ªëng|v√†o)",
            re.IGNORECASE
        )
        
        noun_list = [
            "trung t√¢m th∆∞∆°ng m·∫°i", "khu vui ch∆°i", "b√£i gi·ªØ xe", "b√£i g·ª≠i xe",
            "b·ªánh vi·ªán", "c√¥ng vi√™n", "chung c∆∞", "si√™u th·ªã", "ƒë·ªãa ch·ªâ",
            "cƒÉn h·ªô", "th·ªã x√£", "tr∆∞·ªùng", "ph√≤ng", "qu√°n", "nh√†", "ch·ª£",
            "ti·ªám", "shop", "qu·∫≠n", "huy·ªán", "th√¥n", "x√≥m", "h·ªì b∆°i", "h·ªì",
            "b√£i", "khu", "s·ªë", "ng√µ", "h·∫ªm", "ƒë∆∞·ªùng", "r·∫°p", "ph·ªë",
            "s√¢n", "nh√† h√†ng", "c·ª≠a h√†ng", "qu·∫£ng tr∆∞·ªùng", "chung c∆∞", "t√≤a nh√†"
        ]
        self.noun_pattern = re.compile(
            r"(?:" + "|".join(noun_list) + r")",
            re.IGNORECASE
        )
        
        self.time_cut_markers = [
            " l√∫c ", " v√†o ", " trong ", " ng√†y ", " h√¥m ", " s√°ng ", " tr∆∞a ",
            " chi·ªÅu ", " t·ªëi ", " mai ", " m·ªët ", " tu·∫ßn ", " th√°ng ", " nƒÉm ",
            " th·ª© ", " cn "," m·ªói ", " m·ªçi ", " h·∫±ng "
        ]
    
    def _load_locations(self) -> List[str]:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        data_path = os.path.join(base_dir, "data", "locations.json")
        
        if not os.path.exists(data_path):
            return []
        
        try:
            with open(data_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                locations = [loc for group in data.values() for loc in group]
                return sorted(locations, key=len, reverse=True)
        except Exception:
            return []
    
    def _is_invalid(self, text: str) -> bool:
        text = text.lower().strip()
        
        if re.search(r'\d+\s*(?:h|g|:|p|ph√∫t|gi√¢y|ti·∫øng|am|pm)\b', text):
            return True
        
        if re.search(r'(?:ng√†y|th√°ng|nƒÉm|th·ª©)\s*\d+', text):
            return True
        
        time_words = [
            "h√¥m nay", "ng√†y mai", "m·ªët", "tu·∫ßn", "th√°ng", "nƒÉm", "s√°ng",
            "tr∆∞a", "chi·ªÅu", "t·ªëi", "ƒë√™m", "khuya", "th·ª©", "ch·ªß nh·∫≠t", "cn"
        ]
        if any(re.search(r'\b' + re.escape(w) + r'\b', text) for w in time_words):
            return True
        
        if len(text) < 2 or text.isdigit():
            return True
        
        if any(re.search(r'\b' + re.escape(b) + r'\b', text) for b in self.black_list):
            return True
        
        return False
    
    def _cut_time_tail(self, text: str) -> str:
        lower = " " + text.lower() + " "
        cut_pos = min(
            (lower.find(mark) for mark in self.time_cut_markers if lower.find(mark) != -1),
            default=None
        )
        return text[:cut_pos].strip() if cut_pos is not None else text
    
    def _clean_extracted_text(self, text: str) -> str:
        words = text.split()
        result = []
        
        for i, word in enumerate(words):
            lower = word.lower()
            prev = words[i - 1].lower() if i > 0 else ""
            
            if lower in self.stop_verbs or lower in ["phim", "·∫£nh", "h√¨nh"]:
                allow = False
                
                valid_pairs = {
                    ("b√£i", "g·ª≠i"), ("b√£i", "gi·ªØ"), ("h·ªì", "b∆°i"),
                    ("vui", "ch∆°i"), ("ph√≤ng", "t·∫≠p"), ("s√¢n", "t·∫≠p"),
                    ("trung t√¢m", "t·∫≠p"), ("khu", "t·∫≠p"), ("s√¢n", "ƒë√°"),
                    ("b√£i", "ƒë√°"), ("r·∫°p", "phim"), ("coi", "phim"),
                    ("xem", "phim"), ("ch·ª•p", "·∫£nh"), ("studio", "·∫£nh"),
                    ("ch·ª•p", "h√¨nh"), ("studio", "h√¨nh"), ("th·ªÉ", "h√¨nh"),
                    ("truy·ªÅn", "h√¨nh"), ("m√†n", "h√¨nh"),
                    ("qu√°n", "ƒÉn"), ("nh√†", "h√†ng"), ("s√¢n", "b√≥ng"), 
                    ("s√¢n", "v·∫≠n"), ("s√¢n", "bay"), ("s√¢n", "kh·∫•u"),
                    ("c·ª≠a", "h√†ng"), ("ƒëi·ªÉm", "h·∫πn"), ("n∆°i", "·ªü")
                }
                
                if (prev, lower) in valid_pairs:
                    allow = True
                
                if word[0].isupper() and i > 0:
                    allow = True
                
                if not allow:
                    break
            
            result.append(word)
        
        return " ".join(result).strip()
    
    def _post_process_clean(self, text: str) -> str:
        patterns = [
            r'^(ƒë·ªãa ch·ªâ|v·ªã tr√≠|n∆°i|nh√†|qu√™)\s+(l√†|c·ªßa|n·∫±m|t·∫°i|·ªü)\s+',
            r'^(l√†|c·ªßa|n·∫±m|t·∫°i|·ªü)\s+',
            r'^(c√°i|ng√¥i|chi·∫øc)\s+'
        ]
        for p in patterns:
            text = re.sub(p, '', text, flags=re.IGNORECASE)
        return text.strip()
    
    def extract(self, text: str) -> Optional[str]:
        text_lower = text.lower()
        candidates = []
        
        for place in self.locations_db:
            if re.search(r'\b' + re.escape(place) + r'\b', text_lower):
                candidates.append(place)
        time_stoppers = r"(?:\s+(?:l√∫c|v√†o|ng√†y|h√¥m|s√°ng|tr∆∞a|chi·ªÅu|t·ªëi|ƒë√™m|mai|m·ªët|m·ªói|h√†ng|m·ªçi|h·∫±ng)|$|[.,?!])"
        candidates += re.findall(f"{self.prep_pattern.pattern}\s+(.*?){time_stoppers}", text, re.IGNORECASE)
        candidates += re.findall(f"(?:^|\s)({self.noun_pattern.pattern}\s+.*?){time_stoppers}", text, re.IGNORECASE)
        valid = []
        for raw in candidates:
            loc = self._cut_time_tail(raw)
            loc = self._clean_extracted_text(loc)
            loc = self._post_process_clean(loc)
            loc = loc.strip(" .,?!")
            
            if loc and not self._is_invalid(loc):
                valid.append(loc)
        
        return max(valid, key=len) if valid else None


if __name__ == "__main__":
    parser = LocationParser()
    print("\nüöÄ LOCATION PARSER TEST\n")
    
    test_cases = [
        "thu√™ nh√† ·ªü ng√µ 123 ph·ªë Hu·∫ø",
        "t·∫≠p gym ·ªü ph√≤ng t·∫≠p th·ªÉ h√¨nh",
        "ƒëi si√™u th·ªã l√∫c 9 gi·ªù t·ªëi",
        "gh√© 539/2/9 b√¨nh th·ªõi",
        "ƒêi b∆°i ·ªü h·ªì b∆°i lam s∆°n",
        "G·ª≠i xe ·ªü b√£i gi·ªØ xe r·∫°p phim",
        "ƒÉn u·ªëng t·∫°i qu√°n ph·ªü 24/7",
        "h·ªçc b√†i ·ªü nh√† b·∫°n",
        "l√†m vi·ªác ·ªü c√¥ng ty ABC",
        "t·∫Øm r·ª≠a ·ªü nh√†",
        "ƒëi ch∆°i ·ªü c√¥ng vi√™n 9/10",
        "ƒëi kh√°m b·ªánh vi·ªán ƒëa khoa",
        "ƒëi ƒë√° b√≥ng ·ªü s√¢n t·∫≠p th·ªÉ thao",
        "ƒëi xem phim ·ªü r·∫°p chi·∫øu b√≥ng",
        "S√°ng mai 8h ƒë∆∞a con ƒëi h·ªçc ·ªü tr∆∞·ªùng ti·ªÉu h·ªçc",
        "ƒê√° banh v√†o ng√†y t·∫°i s√¢n hu·ª≥nh ƒë·ª©c"
        ""
    ]
    
    print(f"{'INPUT':<45} | {'OUTPUT'}")
    print("-" * 80)
    for t in test_cases:
        print(f"{t:<45} | {parser.extract(t)}")